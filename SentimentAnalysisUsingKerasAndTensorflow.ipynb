{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "import json\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exacon03/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "training = np.genfromtxt('/path/to/Sentiment Analysis Dataset.csv', delimiter=',', skip_header=1, usecols=(1, 3),max_rows=100000, dtype=None)\n",
    "\n",
    "train_x = [x[1] for x in training]\n",
    "# index all the sentiment labels\n",
    "train_y = np.asarray([x[0] for x in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = 1,'sigmoid'\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "            'ngram_range': (1,2),  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df':  1,\n",
    "            'max_df': 1.0,\n",
    "}\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "train_x = vectorizer.fit_transform(train_x)\n",
    " # Select top 'k' of the vectorized features.\n",
    "selector = SelectKBest(chi2, k=min(20000, train_x.shape[1]))\n",
    "selector.fit(train_x, train_y)\n",
    "# print(train_x)\n",
    "train_x = selector.transform(train_x).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=mlp_model(2,32,0.3,(20000,),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "80000/80000 [==============================] - 19s 238us/step - loss: 0.6508 - acc: 0.6234 - val_loss: 0.5888 - val_acc: 0.7301\n",
      "Epoch 2/20\n",
      "80000/80000 [==============================] - 19s 233us/step - loss: 0.5645 - acc: 0.7266 - val_loss: 0.5216 - val_acc: 0.7663\n",
      "Epoch 3/20\n",
      "80000/80000 [==============================] - 19s 232us/step - loss: 0.5194 - acc: 0.7490 - val_loss: 0.4908 - val_acc: 0.7752\n",
      "Epoch 4/20\n",
      "80000/80000 [==============================] - 18s 229us/step - loss: 0.4916 - acc: 0.7675 - val_loss: 0.4741 - val_acc: 0.7827\n",
      "Epoch 5/20\n",
      "80000/80000 [==============================] - 18s 228us/step - loss: 0.4715 - acc: 0.7796 - val_loss: 0.4625 - val_acc: 0.7880\n",
      "Epoch 6/20\n",
      "80000/80000 [==============================] - 18s 228us/step - loss: 0.4580 - acc: 0.7868 - val_loss: 0.4552 - val_acc: 0.7924\n",
      "Epoch 7/20\n",
      "80000/80000 [==============================] - 19s 232us/step - loss: 0.4472 - acc: 0.7923 - val_loss: 0.4486 - val_acc: 0.7929\n",
      "Epoch 8/20\n",
      "80000/80000 [==============================] - 19s 236us/step - loss: 0.4367 - acc: 0.7986 - val_loss: 0.4452 - val_acc: 0.7962\n",
      "Epoch 9/20\n",
      "80000/80000 [==============================] - 18s 229us/step - loss: 0.4283 - acc: 0.8021 - val_loss: 0.4415 - val_acc: 0.7967\n",
      "Epoch 10/20\n",
      "80000/80000 [==============================] - 19s 234us/step - loss: 0.4247 - acc: 0.8022 - val_loss: 0.4389 - val_acc: 0.7980\n",
      "Epoch 11/20\n",
      "80000/80000 [==============================] - 19s 232us/step - loss: 0.4190 - acc: 0.8044 - val_loss: 0.4372 - val_acc: 0.7988\n",
      "Epoch 12/20\n",
      "80000/80000 [==============================] - 19s 231us/step - loss: 0.4116 - acc: 0.8076 - val_loss: 0.4357 - val_acc: 0.8004\n",
      "Epoch 13/20\n",
      "80000/80000 [==============================] - 18s 227us/step - loss: 0.4077 - acc: 0.8085 - val_loss: 0.4333 - val_acc: 0.8007\n",
      "Epoch 14/20\n",
      "80000/80000 [==============================] - 18s 228us/step - loss: 0.4050 - acc: 0.8098 - val_loss: 0.4326 - val_acc: 0.8014\n",
      "Epoch 15/20\n",
      "80000/80000 [==============================] - 19s 233us/step - loss: 0.4010 - acc: 0.8124 - val_loss: 0.4306 - val_acc: 0.8023\n",
      "Epoch 16/20\n",
      "80000/80000 [==============================] - 19s 234us/step - loss: 0.3977 - acc: 0.8123 - val_loss: 0.4304 - val_acc: 0.8020\n",
      "Epoch 17/20\n",
      "80000/80000 [==============================] - 19s 234us/step - loss: 0.3964 - acc: 0.8126 - val_loss: 0.4312 - val_acc: 0.8022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff466993978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "  batch_size=512,\n",
    "  epochs=20,\n",
    "  verbose=1,\n",
    "  validation_split=0.2,\n",
    "  callbacks = [EarlyStopping(monitor='val_loss', patience=1)],     \n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as json file\n",
    "model_json = model.to_json()\n",
    "with open('VectorModel.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#save weights of models separately\n",
    "model.save_weights('VectorModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your saved model structure\n",
    "json_file = open('VectorModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('VectorModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a sentence to be evaluated, or Enter to quit: helloi\n",
      "Positive sentiment; 29.558897% confidence\n",
      "Input a sentence to be evaluated, or Enter to quit: this is bad\n",
      "Negative sentiment; 98.415571% confidence\n",
      "Input a sentence to be evaluated, or Enter to quit: hello\n",
      "Positive sentiment; 92.687285% confidence\n",
      "Input a sentence to be evaluated, or Enter to quit: hello!\n",
      "Positive sentiment; 92.687285% confidence\n"
     ]
    }
   ],
   "source": [
    "labels= ['Negative','Positive']\n",
    "while 1:\n",
    "    evalSentence =input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "    evalSentence = [evalSentence]\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    "\n",
    "    # format your input for the neural net\n",
    "    tx = vectorizer.transform(evalSentence)\n",
    "    tx = selector.transform(tx).astype('float32')\n",
    "    #run model\n",
    "    pred = model.predict(tx)\n",
    "    print(\"%s sentiment; %f%% confidence\" % (labels[int(np.round(pred[0][0]))], np.abs(pred[0][0]-0.5)*200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
